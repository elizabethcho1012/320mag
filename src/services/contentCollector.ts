// ì½˜í…ì¸  ìˆ˜ì§‘ ë° AI ë¦¬ë¼ì´íŒ… ì„œë¹„ìŠ¤
// RSS, API, ì›¹ ìŠ¤í¬ë˜í•‘ì„ í†µí•´ ì½˜í…ì¸ ë¥¼ ìˆ˜ì§‘í•˜ê³  AI ì—ë””í„°ê°€ ë¦¬ë¼ì´íŒ…í•©ë‹ˆë‹¤.

import { supabase } from '@/lib/supabase';
import { getEditorByCategory } from '@/data/editors';
import { rewriteContent, generateImage, generateChallengeQuestion } from '@/lib/openai';
import type { ContentSourceConfig } from '@/data/content-sources';
import type { RawContentCache } from '@/types/ai-editor';

interface CollectedContent {
  title: string;
  content: string;
  url: string;
  publishedDate: Date;
  author?: string;
}

/**
 * RSS í”¼ë“œì—ì„œ ì½˜í…ì¸  ìˆ˜ì§‘
 */
async function fetchFromRSS(sourceUrl: string): Promise<CollectedContent[]> {
  try {
    // RSS íŒŒì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (ì˜ˆ: rss-parser)
    // ì‹¤ì œ êµ¬í˜„ì‹œ rss-parser ì„¤ì¹˜ í•„ìš”: npm install rss-parser
    const Parser = await import('rss-parser');
    const parser = new Parser.default();

    const feed = await parser.parseURL(sourceUrl);

    return feed.items.slice(0, 10).map((item) => ({
      title: item.title || '',
      content: item.contentSnippet || item.content || '',
      url: item.link || '',
      publishedDate: item.pubDate ? new Date(item.pubDate) : new Date(),
      author: item.creator || item.author,
    }));
  } catch (error) {
    console.error('Error fetching RSS:', error);
    return [];
  }
}

/**
 * ì›¹ í˜ì´ì§€ì—ì„œ ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
 */
async function fetchFromWeb(sourceUrl: string): Promise<CollectedContent[]> {
  try {
    // ì‹¤ì œ êµ¬í˜„ì‹œ ì›¹ ìŠ¤í¬ë˜í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš” (ì˜ˆ: cheerio, puppeteer)
    // ë˜ëŠ” ì„œë²„ì‚¬ì´ë“œì—ì„œ ì²˜ë¦¬

    // ì—¬ê¸°ì„œëŠ” í”Œë ˆì´ìŠ¤í™€ë” ë°ì´í„° ë°˜í™˜
    console.log(`Web scraping from ${sourceUrl} - ì‹¤ì œ êµ¬í˜„ í•„ìš”`);
    return [];
  } catch (error) {
    console.error('Error fetching from web:', error);
    return [];
  }
}

/**
 * íŠ¹ì • ì†ŒìŠ¤ì—ì„œ ì½˜í…ì¸  ìˆ˜ì§‘
 */
export async function collectFromSource(
  source: ContentSourceConfig
): Promise<CollectedContent[]> {
  console.log(`Collecting from ${source.name} (${source.type})...`);

  let contents: CollectedContent[] = [];

  switch (source.type) {
    case 'rss':
      contents = await fetchFromRSS(source.url);
      break;
    case 'web':
      contents = await fetchFromWeb(source.url);
      break;
    case 'api':
      // API ìˆ˜ì§‘ ë¡œì§ (ì†ŒìŠ¤ë³„ë¡œ ë‹¤ë¦„)
      break;
  }

  return contents;
}

/**
 * ìˆ˜ì§‘í•œ ì›ë³¸ ì½˜í…ì¸ ë¥¼ DBì— ìºì‹±
 */
async function cacheRawContent(
  sourceId: string,
  content: CollectedContent,
  category: string
): Promise<string> {
  try {
    // ì¤‘ë³µ ì²´í¬
    const { data: existing } = await supabase
      .from('raw_content_cache')
      .select('id')
      .eq('original_url', content.url)
      .single();

    if (existing) {
      console.log(`Content already cached: ${content.url}`);
      return existing.id;
    }

    // ìƒˆ ì½˜í…ì¸  ì €ì¥
    const { data, error } = await supabase
      .from('raw_content_cache')
      .insert({
        source_id: sourceId,
        original_url: content.url,
        title: content.title,
        content: content.content,
        published_date: content.publishedDate.toISOString(),
        author: content.author,
        category,
        is_processed: false,
      })
      .select()
      .single();

    if (error) throw error;

    return data.id;
  } catch (error) {
    console.error('Error caching raw content:', error);
    throw error;
  }
}

/**
 * ì›ë³¸ ì½˜í…ì¸ ë¥¼ AI ì—ë””í„° ìŠ¤íƒ€ì¼ë¡œ ë¦¬ë¼ì´íŒ…í•˜ì—¬ ë°œí–‰
 */
export async function processAndPublishContent(
  rawContent: RawContentCache
): Promise<string | null> {
  try {
    const editor = getEditorByCategory(rawContent.category);

    if (!editor) {
      console.error(`No editor found for category: ${rawContent.category}`);
      return null;
    }

    console.log(`Processing content with ${editor.name} (${editor.category})...`);

    // 1. AI ë¦¬ë¼ì´íŒ…
    const rewritten = await rewriteContent({
      originalContent: rawContent.content,
      editorPromptTemplate: editor.promptTemplate,
      title: rawContent.title,
    });

    // 2. AI ì´ë¯¸ì§€ ìƒì„±
    let aiImageUrl: string | null = null;
    try {
      aiImageUrl = await generateImage({
        title: rewritten.title,
        content: rewritten.content,
        category: editor.category,
      });
    } catch (error) {
      console.error('Error generating image:', error);
      // ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
    }

    // 3. ì±Œë¦°ì§€ ì§ˆë¬¸ ìƒì„±
    const challengeQuestion = await generateChallengeQuestion(
      rewritten.title,
      rewritten.content
    );

    // 4. ì•„í‹°í´ ë°œí–‰
    const { data: article, error } = await supabase
      .from('articles')
      .insert({
        title: rewritten.title,
        slug: generateSlug(rewritten.title),
        excerpt: rewritten.excerpt,
        content: rewritten.content,
        author_name: editor.name,
        ai_editor_id: editor.id,
        source_urls: [rawContent.original_url],
        ai_generated_image: aiImageUrl,
        challenge_question: challengeQuestion,
        is_ai_generated: true,
        status: 'published',
        published_at: new Date().toISOString(),
        category_id: getCategoryId(editor.category), // ì¹´í…Œê³ ë¦¬ ID ë§¤í•‘ í•„ìš”
      })
      .select()
      .single();

    if (error) throw error;

    // 5. ì›ë³¸ ì½˜í…ì¸ ë¥¼ ì²˜ë¦¬ ì™„ë£Œë¡œ í‘œì‹œ
    await supabase
      .from('raw_content_cache')
      .update({
        is_processed: true,
        ai_rewritten_article_id: article.id,
      })
      .eq('id', rawContent.id);

    console.log(`âœ… Published: ${rewritten.title} by ${editor.name}`);
    return article.id;
  } catch (error) {
    console.error('Error processing content:', error);
    return null;
  }
}

/**
 * ì „ì²´ ìë™í™” í”„ë¡œì„¸ìŠ¤: ìˆ˜ì§‘ â†’ ì²˜ë¦¬ â†’ ë°œí–‰
 */
export async function runDailyContentPipeline(
  sources: ContentSourceConfig[]
): Promise<void> {
  console.log('ğŸš€ Starting daily content pipeline...');

  for (const source of sources) {
    try {
      // 1. ì½˜í…ì¸  ìˆ˜ì§‘
      const contents = await collectFromSource(source);
      console.log(`Collected ${contents.length} items from ${source.name}`);

      // 2. ì›ë³¸ ìºì‹±
      const cachedIds: string[] = [];
      for (const content of contents) {
        const id = await cacheRawContent(source.id, content, source.category);
        cachedIds.push(id);
      }

      // 3. ë¯¸ì²˜ë¦¬ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
      const { data: unprocessed, error } = await supabase
        .from('raw_content_cache')
        .select('*')
        .eq('is_processed', false)
        .eq('category', source.category)
        .limit(5); // ì¹´í…Œê³ ë¦¬ë‹¹ í•˜ë£¨ 5ê°œê¹Œì§€

      if (error) throw error;

      // 4. AI ì²˜ë¦¬ ë° ë°œí–‰
      for (const raw of unprocessed || []) {
        await processAndPublishContent(raw);
        // API ë ˆì´íŠ¸ ë¦¬ë°‹ ê³ ë ¤í•˜ì—¬ ë”œë ˆì´
        await new Promise((resolve) => setTimeout(resolve, 2000));
      }
    } catch (error) {
      console.error(`Error processing source ${source.name}:`, error);
      continue;
    }
  }

  console.log('âœ… Daily content pipeline completed!');
}

// ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
function generateSlug(title: string): string {
  return title
    .toLowerCase()
    .replace(/[^a-z0-9ê°€-í£]+/g, '-')
    .replace(/^-+|-+$/g, '');
}

function getCategoryId(categoryName: string): string {
  // ì‹¤ì œë¡œëŠ” DBì—ì„œ ì¹´í…Œê³ ë¦¬ IDë¥¼ ì¡°íšŒí•´ì•¼ í•¨
  // ì—¬ê¸°ì„œëŠ” í”Œë ˆì´ìŠ¤í™€ë”
  return categoryName;
}
